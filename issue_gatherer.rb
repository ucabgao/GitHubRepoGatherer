#!/usr/bin/env ruby

require 'net/http'
require 'json'
require "FileUtils"

KEYWORD_FILE = 'keywords.txt'
REPO_FILE = 'repos.txt'
URL_PREFIX= 'https://api.github.com/search/issues?'
SEARCH_TERMS = 'in:title,body+language:javascript+state:closed'
SEARCH_PARAMETERS = '&sort=created&order=asc&per_page=100'

TOKEN = '0d87f5b4b79a37a1ea0012d9188c28b81001a214' # Generated by GitHub, use your own
PASSWORD = 'x-oauth-basic' # Format set by GitHub

MAXIMUM_PAGE = 10 # GitHub API provides up to 1000 results per query
SAMPLE_NUMBER = 40
SLEEP_TIME = 3 # GitHub API allows up to 20 queries per minute

class IssueGatherer

  attr_accessor :total_pages, :keywords, :repos

  def initialize
    @keywords = []
    @repos = []
    File.readlines(KEYWORD_FILE).each do |line|
      @keywords.push(line.chomp.gsub(/\s/, '%20'))
    end
    @total_pages = Hash.new
    @keywords.each { |keyword| @total_pages[keyword] = 1 }
  end

  def write_file(file, array)
    file = File.new(file, 'a')
    array.uniq.each { |element| file.write(element) }
    file.close
  end

  def query(search_url)
    uri = URI(search_url)
    response = Net::HTTP.start(uri.host, uri.port,
                               :use_ssl => uri.scheme == 'https') do |http|
      request = Net::HTTP::Get.new uri

      # Basic authentication for GitHub API
      request.basic_auth TOKEN, PASSWORD
      http.request request
    end

    # Status
    puts response.code       # => '200'
    puts response.message    # => 'OK'
    puts response.class.name # => 'HTTPOK'

    sleep(SLEEP_TIME)

    return JSON.parse(response.body)
  end

  # Estimate how many pages of results are there for each keyword search,
  # assuming 100 results per page
  def estimate_page_number
    @keywords.each do |keyword|
      total_count = query(build_url(keyword))['total_count']

      if total_count % 100 == 0
        @total_pages[keyword] = total_count/100
      else
        @total_pages[keyword] = total_count/100 + 1
      end
    end

    @total_pages.each { |key, value| puts key + ': ' + value.to_s }
  end

  def build_url(keyword, opts = {})
    page = opts[:page] || 1
    search_url =
        URL_PREFIX + "q=#{keyword}+" + SEARCH_TERMS + SEARCH_PARAMETERS + "&page=#{page.to_s}"
  end

  def gather
    @keywords.each do |keyword|
      page_number =
          @total_pages[keyword] >= MAXIMUM_PAGE ? MAXIMUM_PAGE : @total_pages[keyword]

      (1..page_number).each do |page|
        issue_hash = query(build_url(keyword, :page => page))

        issue_hash['items'].each do |repo|
          repo_url =
              repo['url'].sub(/api/, 'www').sub(/\/repos/, '').sub(/\/issues.*/, '') + "\n"
          @repos.push(repo_url)
        end
      end
    end

    write_file(REPO_FILE, repos)
    write_file(SAMPLE_FILE, repos.shuffle.slice(0, SAMPLE_NUMBER))
  end
end

repo_gatherer = IssueGatherer.new
repo_gatherer.estimate_page_number
repo_gatherer.gather